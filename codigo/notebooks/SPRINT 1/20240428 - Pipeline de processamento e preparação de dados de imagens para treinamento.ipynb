{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline de processamento e preparação de dados de imagens para treinamento\n",
    "\n",
    "Este notebook unifica todo o código encontrado nos arquivos **\"20240425 - KMeans Agro.ipynb\"** e **\"20240426_Pipeline_Básica_de_Processamento_de_Imagem.ipynb\"**. Esses arquivos apresentam duas implementações distintas, porém valiosas para o projeto. Neste momento inicial, optamos por manter essas duas abordagens diferentes para realizar testes na Sprint 2 com as modelagens iniciais das Redes Neurais Convolucionais. Contudo, é esperado que na próxima Sprint 2, unifiquemos esse processo em um só.\n",
    "\n",
    "Com o objetivo de preservar a inteligibilidade deste arquivo, optamos por manter os testes unitários exclusivamente em seus respectivos arquivos, Kmeans Agro e Pipeline Básica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementação utilizando KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import tqdm\n",
    "\n",
    "class KMeansImageProcessingPipeline:\n",
    "\tdef __init__(self, base_dir):\n",
    "\t\tself.base_dir = base_dir\n",
    "    \n",
    "\tdef read_and_process_image(self, path):\n",
    "\t\timg = cv2.imread(path)\n",
    "\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\t\treturn img\n",
    "\n",
    "\tdef kmeans_image(self, img, k, attempts=10):\n",
    "\t\t# Redimensionar a imagem para um vetor 1D\n",
    "\t\tvectorized_img = img.reshape((-1,3))\n",
    "\t\tvectorized_img = np.float32(vectorized_img)\n",
    "\n",
    "\t\t# Definir critérios para o algoritmo K-means\n",
    "\t\tcriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "\n",
    "\t\t# Executar o algoritmo K-means\n",
    "\t\tret, label, center = cv2.kmeans(vectorized_img, k, None, criteria, attempts, cv2.KMEANS_PP_CENTERS)\n",
    "\t\tcenter = np.uint8(center)\n",
    "\n",
    "\t\t# Recriar a imagem a partir dos clusters encontrados\n",
    "\t\tres = center[label.flatten()]\n",
    "\t\tresult_image = res.reshape((img.shape))\n",
    "\n",
    "\t\treturn result_image\n",
    "\n",
    "\tdef sharpen_image(self, img):\n",
    "\t\t# Cria um kernel para afiar a imagem\n",
    "\t\tkernel = np.array([[-1,-1,-1],\n",
    "\t\t\t\t\t\t\t\t\t\t\t[-1,9,-1],\n",
    "\t\t\t\t\t\t\t\t\t\t\t[-1,-1,-1]])\n",
    "\n",
    "\t\t# Aplica o kernel na imagem usando filter2D\n",
    "\t\tsharpened_image = cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "\t\treturn sharpened_image\n",
    "\n",
    "\tdef process_all_images(self, k, attempts=10):\n",
    "\t\tprocessed_images = []\n",
    "\t\t\t\n",
    "\t\timages = os.listdir(self.base_dir)\n",
    "\n",
    "\t\tfor image_name in tqdm.tqdm(images):\n",
    "\t\t\tpath = os.path.join(self.base_dir, image_name)\n",
    "\t\t\timg = self.read_and_process_image(path)\n",
    "\n",
    "\t\t\t# Aplica o algoritmo de k-means na imagem\n",
    "\t\t\tkmeans_img = self.kmeans_image(img, k, attempts)\n",
    "\t\t\t\n",
    "\t\t\t# Afia a imagem resultante\n",
    "\t\t\tsharpened_img = self.sharpen_image(kmeans_img)\n",
    "\n",
    "\t\t\t# Adiciona a imagem afiada à lista de retorno\n",
    "\t\t\tprocessed_images.append(sharpened_img)\n",
    "\n",
    "\t\treturn processed_images\n",
    "\n",
    "\tdef analyze_kmeans_inertia(self, k_range):\n",
    "\t\tsample_image_path = os.path.join(self.base_dir, os.listdir(self.base_dir)[0])\n",
    "\t\timg = self.read_and_process_image(sample_image_path)\n",
    "\t\tvectorized_img = img.reshape((-1, 3))\n",
    "\t\tvectorized_img = np.float32(vectorized_img)\n",
    "\t\t\n",
    "\t\tinertias = []\n",
    "\t\tfor k in k_range:\n",
    "\t\t\t\tkmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\t\t\t\tkmeans.fit(vectorized_img)\n",
    "\t\t\t\tinertias.append(kmeans.inertia_)\n",
    "\t\t\n",
    "\t\tplt.figure(figsize=[20, 5])\n",
    "\t\tplt.subplot(1, 2, 1)\n",
    "\t\tplt.plot(k_range, inertias, \"-o\")\n",
    "\t\tplt.xlabel(\"$k$\", fontsize=14)\n",
    "\t\tplt.ylabel(\"Inertia\", fontsize=14)\n",
    "\t\tplt.grid(True)\n",
    "\t\t\n",
    "\t\tplt.subplot(1, 2, 2)\n",
    "\t\tplt.plot(k_range[:-1], np.diff(inertias), \"-o\")\n",
    "\t\tplt.xlabel(\"$k$\", fontsize=14)\n",
    "\t\tplt.ylabel(\"Change in inertia\", fontsize=14)\n",
    "\t\tplt.grid(True)\n",
    "\t\tplt.show()\n",
    "\n",
    "# Usage\n",
    "base_dir = '../../data/dataset_inteli_test/tci_pngs'\n",
    "pipeline = KMeansImageProcessingPipeline(base_dir)\n",
    "processed_images = pipeline.process_all_images(5)  # Change k as needed\n",
    "pipeline.analyze_kmeans_inertia(range(4, 16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in processed_images:\n",
    "\t\tplt.imshow(img)\n",
    "\t\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais informações e testes unitários em: [Arquivo KMeans](../notebooks/20240425%20-%20KMeans%20Agro.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualização de imagens: PCA e Média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class ImageVisualization:\n",
    "  def __init__(self, image_paths):\n",
    "    self.images = []\n",
    "    self.target_size = (1200, 1200)\n",
    "\n",
    "    for path in image_paths:\n",
    "      image = self.load_image(path)\n",
    "\n",
    "      if image is not None:\n",
    "        self.images.append(image)\n",
    "\n",
    "  def load_image(self, path):\n",
    "    image = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    if image is None:\n",
    "      print(f\"Failed to load image: {path}\")\n",
    "      return None\n",
    "\n",
    "    if image.shape[:2] != self.target_size:\n",
    "      image = cv2.resize(image, self.target_size[::-1])\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "  def merge_images(self):\n",
    "    if not self.images:\n",
    "      raise ValueError(\"Nenhuma imagem foi carregada no pipeline.\")\n",
    "\n",
    "    # Cada pixel na imagem resultante é a média dos pixels correspondentes de todas as imagens.\n",
    "    merged_image = np.mean(self.images, axis=0, dtype=np.float32)\n",
    "    \n",
    "    # normalizando para valores de 0 a 255\n",
    "    merged_image = cv2.normalize(merged_image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "    return merged_image\n",
    "\n",
    "  def merge_images_pca(self, n_components=3):\n",
    "    if not self.images:\n",
    "      raise ValueError(\"Nenhuma imagem foi carregada no pipeline.\")\n",
    "\n",
    "    # Empilhar todas as imagens em uma matriz 2D (pixels x bandas)\n",
    "    data = np.stack([img.ravel() for img in self.images], axis=-1)\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(data)\n",
    "\n",
    "    merged_image = principal_components.reshape(self.images[0].shape[:2] + (n_components,))\n",
    "    merged_image = cv2.normalize(merged_image, None, 0, 255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    return merged_image\n",
    "\n",
    "  # O método abaixo permite visualizar a imagem com a resulução de seu tamanho\n",
    "  def view_image(self, image):\n",
    "    dpi = 100  \n",
    "    height, width = image.shape[:2]\n",
    "    figsize = width / float(dpi), height / float(dpi)  \n",
    "\n",
    "    plt.figure(figsize=figsize, dpi=dpi)  \n",
    "\n",
    "    if image.ndim == 2 or image.shape[2] == 1:\n",
    "      plt.imshow(image, cmap='gray')  \n",
    "\n",
    "    else:\n",
    "      plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Criando imagens para utilizar na etapa 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagens Utilizando o método merge_images\n",
    "image_paths = ['../../data/dataset_inteli_test/images/595_2019-8-14_S2L1C_21JYJ/b11.tif', '../../data/dataset_inteli_test/images/595_2019-8-14_S2L1C_21JYJ/b12.tif', '../../data/dataset_inteli_test/images/595_2019-8-14_S2L1C_21JYJ/b2.tif', '../../data/dataset_inteli_test/images/595_2019-8-14_S2L1C_21JYJ/b3.tif', '../../data/dataset_inteli_test/images/595_2019-8-14_S2L1C_21JYJ/b4.tif', '../../data/dataset_inteli_test/images/595_2019-8-14_S2L1C_21JYJ/b5.tif', '../../data/dataset_inteli_test/images/595_2019-8-14_S2L1C_21JYJ/b6.tif', '../../data/dataset_inteli_test/images/595_2019-8-14_S2L1C_21JYJ/b7.tif', '../../data/dataset_inteli_test/images/595_2019-8-14_S2L1C_21JYJ/b8.tif', '../../data/dataset_inteli_test/images/595_2019-8-14_S2L1C_21JYJ/b8a.tif' ]\n",
    "\n",
    "image_viz = ImageVisualization(image_paths)\n",
    "\n",
    "merged_image_mean = image_viz.merge_images()\n",
    "image_viz.view_image(merged_image_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagens utilizando o método merge_images_pca\n",
    "merged_image_pca = image_viz.merge_images_pca()\n",
    "image_viz.view_image(merged_image_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline de processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ImageProcessingPipeline:\n",
    "\tdef __init__(self, images):\n",
    "\t\tself.images = images  \n",
    "\n",
    "\t@staticmethod\n",
    "\tdef normalize_image(image):\n",
    "\t\treturn image / 255.0\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef resize_image(image, target_size=(1200, 1200)):\n",
    "\t\treturn cv2.resize(image, target_size)\n",
    "\n",
    "\tdef apply_clahe(self, image):\n",
    "\t\tclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "\t\treturn clahe.apply(image.astype(np.uint8))\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef crop_image(image, crop_size=(200, 200)):\n",
    "\t\tcrops = []\n",
    "\n",
    "\t\tfor i in range(0, image.shape[0], crop_size[0]):\n",
    "\n",
    "\t\t\tfor j in range(0, image.shape[1], crop_size[1]):\n",
    "\t\t\t\t\tcrop = image[i:i+crop_size[0], j:j+crop_size[1]]\n",
    "\n",
    "\t\t\t\t\tif crop.shape[0] == crop_size[0] and crop.shape[1] == crop_size[1]:\n",
    "\t\t\t\t\t\tcrops.append(crop)\n",
    "\n",
    "\t\treturn crops\n",
    "\n",
    "\tdef augment_images(self, image):\n",
    "\t\taug_images = []\n",
    "\n",
    "\t\tfor angle in [0, 90, 180, 270]:\n",
    "\t\t\trotated = self.rotate_image(image, angle)\n",
    "\t\t\taug_images.append(rotated)\n",
    "\t\t\taug_images.append(cv2.flip(rotated, 1))\n",
    "\t\t\t\n",
    "\t\treturn aug_images\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef rotate_image(image, angle):\n",
    "\t\t(h, w) = image.shape[:2]\n",
    "\t\tcenter = (w // 2, h // 2)\n",
    "\n",
    "\t\tM = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "\t\treturn cv2.warpAffine(image, M, (w, h))\n",
    "\n",
    "\tdef process_images(self):\n",
    "\t\tprocessed_images = []\n",
    "\n",
    "\t\tfor img in self.images:\n",
    "\t\t\tif img.ndim == 1:\n",
    "\t\t\t\tclahe_img = self.apply_clahe(img)\n",
    "\t\t\telse:\n",
    "\t\t\t\tclahe_img = img\n",
    "\t\t\t\n",
    "\t\t\tnorm_img = ImageProcessingPipeline.normalize_image(clahe_img)\n",
    "\t\t\tresized_img = ImageProcessingPipeline.resize_image(norm_img)\n",
    "\n",
    "\t\t\tcropped_images = self.crop_image(resized_img)\n",
    "\n",
    "\t\t\tfor crop in cropped_images:\n",
    "\t\t\t\taugmented_imgs = self.augment_images(crop)\n",
    "\t\t\t\tprocessed_images.extend(augmented_imgs)\n",
    "\n",
    "\t\treturn processed_images\n",
    "\n",
    "\tdef show_image(self, image):\n",
    "\t\t\tplt.imshow(image, cmap='gray')\n",
    "\t\t\tplt.axis('off')\n",
    "\t\t\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Aplicando a pipeline para imagens geradas através do KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ImageProcessingPipeline(processed_images)\n",
    "processed_images_pipeline = pipeline.process_images()\n",
    "\n",
    "# Mostrar algumas das imagens processadas\n",
    "for img in processed_images_pipeline[:8]:  # Mostra as primeiras 8 imagens processadas\n",
    "    pipeline.show_image(img)\n",
    "\n",
    "len(processed_images_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Aplicando a pipeline para imagens geradas através da média dos pixeis entre bandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_mean_image = ImageProcessingPipeline([merged_image_mean])\n",
    "processed_images_pipeline_2 = pipeline_mean_image.process_images()\n",
    "\n",
    "# Mostrar algumas das imagens processadas\n",
    "for img in processed_images_pipeline_2[:8]:  # Mostra as primeiras 8 imagens processadas\n",
    "    pipeline.show_image(img)\n",
    "\n",
    "len(processed_images_pipeline_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Aplicando a pipeline para imagens geradas a partir do PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pca_image = ImageProcessingPipeline([merged_image_pca])\n",
    "processed_images_pipeline_3 = pipeline_pca_image.process_images()\n",
    "\n",
    "# Mostrar algumas das imagens processadas\n",
    "for img in processed_images_pipeline_3[:8]:  # Mostra as primeiras 8 imagens processadas\n",
    "    pipeline.show_image(img)\n",
    "\n",
    "len(processed_images_pipeline_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais informações e testes unitários em: [Arquivo Pipeline Básica](../notebooks/20240426_Pipeline_Básica_de_Processamento_de_Imagem.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
