{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQl90jW20q1D"
      },
      "source": [
        "# Obter dados no drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-LUSzEzpW-U",
        "outputId": "517272de-0dca-4151-cb3c-343421ad75e7"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_model_optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehPwUJLzSsLC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras import layers, regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from sklearn.model_selection import ParameterGrid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibpCXTgmS_Hi",
        "outputId": "d840e26f-59b9-46ae-8533-785161a2dbc7"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xSP-0NwWdAq"
      },
      "source": [
        "# Carregamento de dados - Sem Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz6Rn0h26UVx"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "masks = []\n",
        "\n",
        "for path in glob('/content/drive/Shared drives/Grupo T de Tech/Data/dataset_inteli/cropped_images/*/*'):\n",
        "  images.append(path + '/image.tif')\n",
        "  masks.append(path + '/mask.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgFIh5hrObFb"
      },
      "outputs": [],
      "source": [
        "# Função para carregar e pré-processar uma imagem e sua máscara\n",
        "def load_and_preprocess_image(image_path, mask_path, target_size):\n",
        "\n",
        "    image = load_img(image_path, target_size=target_size)\n",
        "    image = img_to_array(image) / 255.0  # Normalização entre 0 e 1\n",
        "\n",
        "    mask = load_img(mask_path, target_size=target_size, color_mode='grayscale')\n",
        "    mask = img_to_array(mask) / 255.0  # Normalização entre 0 e 1\n",
        "\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qu_4ZJaKOTGF"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    # Lista para armazenar imagens e máscaras pré-processadas\n",
        "    images_processed = []\n",
        "    masks_processed = []\n",
        "\n",
        "    # Carregar e pré-processar todas as imagens e máscaras\n",
        "    for img_path, mask_path in zip(images, masks):\n",
        "        img, mask = load_and_preprocess_image(img_path, mask_path, target_size=(256, 256))\n",
        "        images_processed.append(img)\n",
        "        masks_processed.append(mask)\n",
        "\n",
        "    # Converter para arrays numpy\n",
        "    images_processed = np.array(images_processed)\n",
        "    masks_processed = np.array(masks_processed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kRApOxtFQ3IX",
        "outputId": "a4c8374a-d9c6-47e2-f456-5cb582c97dcf"
      },
      "outputs": [],
      "source": [
        "images_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1XJsmj6Iu1W"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(images_processed, masks_processed, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dApijRqyWk_n"
      },
      "source": [
        "# Definição de parâmetros e modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JcNGKwDxX94"
      },
      "outputs": [],
      "source": [
        "class CyclicLR(Callback):\n",
        "    def __init__(self, base_lr=1e-4, max_lr=1e-3, step_size=2000., mode='triangular'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.iterations = 0\n",
        "        self.history = {}\n",
        "\n",
        "    def clr(self):\n",
        "        cycle = np.floor(1 + self.iterations / (2 * self.step_size))\n",
        "        x = np.abs(self.iterations / self.step_size - 2 * cycle + 1)\n",
        "        lr = self.base_lr + (self.max_lr - self.base_lr) * max(0, (1 - x))\n",
        "        if self.mode == 'triangular2':\n",
        "            lr = lr / float(2 ** (cycle - 1))\n",
        "        elif self.mode == 'exp_range':\n",
        "            lr = lr * (0.999 ** self.iterations)\n",
        "        return lr\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        logs = logs or {}\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        self.iterations += 1\n",
        "        lr = self.clr()\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
        "        self.history.setdefault('lr', []).append(lr)\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "# Função de callbacks\n",
        "def get_callbacks():\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "    clr = CyclicLR(base_lr=1e-4, max_lr=1e-3, step_size=2000., mode='triangular2')\n",
        "    #pruning = sparsity.UpdatePruningStep()\n",
        "    #return [pruning, early_stopping, reduce_lr, clr]\n",
        "    return [early_stopping, reduce_lr, clr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZMBQc7JL15q"
      },
      "outputs": [],
      "source": [
        "# Função para calcular a sigmoide e converter para 0 ou 1 o output\n",
        "class ThresholdLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        return tf.where(inputs < 0.5, 0.0, 1.0)\n",
        "\n",
        "# Função para calcular o Dice Coefficient\n",
        "def dice_coefficient(y_train, y_val):\n",
        "    smooth = 1e-6\n",
        "    intersection = tf.reduce_sum(y_train * y_val)\n",
        "    dice_coefficient = (2. * intersection + smooth) / (tf.reduce_sum(y_train) + tf.reduce_sum(y_val) + smooth)\n",
        "    return dice_coefficient\n",
        "\n",
        "# Função de perda de Dice\n",
        "def dice_loss(y_train, y_val):\n",
        "    return 1 - dice_coefficient(y_train, y_val)\n",
        "\n",
        "# Função para calcular a penalidade adicional\n",
        "def penalty_loss(y_train, y_val, penalty_weight):\n",
        "    # Calcular a penalidade considerando a diferença entre y_train e y_val\n",
        "    penalty = tf.reduce_sum(tf.abs(y_train - y_val))\n",
        "    # Multiplicar a penalidade pelo peso da penalidade\n",
        "    weighted_penalty = penalty_weight * penalty\n",
        "    return weighted_penalty\n",
        "\n",
        "# Função de perda combinada\n",
        "def combined_loss(y_train, y_val, alpha, beta, gamma, penalty_weight):\n",
        "    # Perda padrão (por exemplo, perda de entropia cruzada binária)\n",
        "    standard_loss = tf.keras.losses.binary_crossentropy(y_train, y_val)\n",
        "    dice = dice_loss(y_train, y_val) # Dice Loss\n",
        "    penalty = penalty_loss(y_train, y_val, penalty_weight) # Penalidade adicional\n",
        "    # Perda total = perda padrão + penalidade + Dice Loss\n",
        "    total_loss = alpha * standard_loss + beta * dice + gamma * penalty\n",
        "    return total_loss\n",
        "\n",
        "# Métrica de acurácia customizada\n",
        "def custom_accuracy(y_train, y_val):\n",
        "    # Calcular a acurácia considerando uma tolerância de 0.5 na predição\n",
        "    y_val_binary = tf.round(y_val)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_train, y_val_binary), tf.float32))\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvDKB-cBmI6L",
        "outputId": "995e43f9-88ab-40a7-d011-5cb587a7bbad"
      },
      "outputs": [],
      "source": [
        "# Carregar o modelo MobileNetV2 pré-treinado\n",
        "pre_trained_model = MobileNetV2(weights='imagenet', include_top=False)\n",
        "\n",
        "# Visualizar a estrutura do modelo\n",
        "pre_trained_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJk0H6D6T9F1"
      },
      "outputs": [],
      "source": [
        "class UNet:\n",
        "    def __init__(self, input_shape, num_filters, kernel_size, dropout_rate, val_reg):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.val_reg = val_reg\n",
        "\n",
        "    def build_model(self):\n",
        "        inputs = tf.keras.Input(shape=self.input_shape)\n",
        "        reg = regularizers.L2(self.val_reg)\n",
        "\n",
        "        # Encoder (contraction path)\n",
        "        base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "        base_model.trainable = False\n",
        "\n",
        "        # Extract specific layers for connections\n",
        "        conv2 = base_model.get_layer('block_1_expand').output\n",
        "        conv3 = base_model.get_layer('block_3_expand').output\n",
        "        decoded = base_model.get_layer('block_6_expand').output\n",
        "\n",
        "        # Decoder (expansion path)\n",
        "        up5 = layers.Conv2DTranspose(self.num_filters[2], (2, 2), strides=(2, 2), padding='same')(decoded)\n",
        "        merge5 = layers.concatenate([conv3, up5], axis=3)\n",
        "        conv5 = layers.Conv2D(self.num_filters[2], self.kernel_size, activation='relu', padding='same')(merge5)\n",
        "        conv5 = layers.Conv2D(self.num_filters[2], self.kernel_size, activation='relu', padding='same')(conv5)\n",
        "\n",
        "        up6 = layers.Conv2DTranspose(self.num_filters[1], (2, 2), strides=(2, 2), padding='same')(conv5)\n",
        "        merge6 = layers.concatenate([conv2, up6], axis=3)\n",
        "        conv6 = layers.Conv2D(self.num_filters[1], self.kernel_size, activation='relu', padding='same')(merge6)\n",
        "        conv6 = layers.Conv2D(self.num_filters[1], self.kernel_size, activation='relu', padding='same')(conv6)\n",
        "\n",
        "        up7 = layers.Conv2DTranspose(self.num_filters[0], (2, 2), strides=(2, 2), padding='same')(conv6)\n",
        "        conv7 = layers.Conv2D(self.num_filters[0], self.kernel_size, activation='relu', padding='same')(up7)\n",
        "        conv7 = layers.Conv2D(self.num_filters[0], self.kernel_size, activation='relu', padding='same')(conv7)\n",
        "\n",
        "        outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(conv7)  # Saída com um canal (máscara binária)\n",
        "\n",
        "        threshold_output = ThresholdLayer()(outputs)\n",
        "\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def compile_and_train(self, X_train, y_train, X_val, y_val, max_epochs, batch_size, alpha, beta, gamma, penalty_weight):\n",
        "        model = self.build_model()\n",
        "        callbacks = get_callbacks()\n",
        "\n",
        "        # Compilar o modelo\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                      loss=lambda y_train, y_val: combined_loss(y_train, y_val, alpha, beta, gamma, penalty_weight),\n",
        "                      metrics=[custom_accuracy])\n",
        "\n",
        "        #model.summary()\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Treinar o modelo\n",
        "        H = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                      epochs=max_epochs, batch_size=batch_size, callbacks=callbacks)\n",
        "\n",
        "        # Salvar o tempo de treino\n",
        "        training_time_gpu = time.time() - start_time\n",
        "\n",
        "        return H, training_time_gpu, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0HalkJyHT-9"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_val, y_val, H, training_time_gpu, max_epochs):\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    results = model.evaluate(X_val, y_val)\n",
        "\n",
        "    inference_time_gpu = time.time() - start_time\n",
        "\n",
        "    print(\"Test Loss:\", results[0])\n",
        "    print(\"Test Accuracy:\", results[1])\n",
        "\n",
        "    # Prever máscaras usando o modelo\n",
        "    predicted_masks = model.predict(X_val)\n",
        "\n",
        "    # Obter métricas de precisão e perda do treinamento\n",
        "    acc = H.history['custom_accuracy']\n",
        "    val_acc = H.history['val_custom_accuracy']\n",
        "    loss = H.history['loss']\n",
        "    val_loss = H.history['val_loss']\n",
        "\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    # Plotar precisão do conjunto\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, acc, 'r', label='Precisão do Conjunto de Treino')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Precisão do Conjunto de Validação')\n",
        "    plt.title('Precisão do Conjunto de Treino e Validação')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Precisão')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotar perda do conjunto\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, loss, 'r', label='Perda do Conjunto de Treino')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Perda do Conjunto de Validação')\n",
        "    plt.title('Perda do Conjunto de Treino e Validação')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Perda')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print('Tempo de treino (segundos): ', training_time_gpu)\n",
        "    print('Tempo de treino por época (segundos): ', training_time_gpu / max_epochs)\n",
        "    print('Tempo de inferência (segundos): ', inference_time_gpu)\n",
        "\n",
        "    return predicted_masks, inference_time_gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RGkYbGK_Uzt3",
        "outputId": "e3e8d2aa-8d6d-452c-c6cd-b882f97d86b8"
      },
      "outputs": [],
      "source": [
        "# Grid de parâmetros para busca\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 1.0, 10.0],\n",
        "    'beta': [0.1, 1.0],\n",
        "    'gamma': [0.0001, 0.001, 0.01],\n",
        "    'penalty_weight': [0.0001, 0.001]\n",
        "}\n",
        "\n",
        "grid = ParameterGrid(param_grid)\n",
        "\n",
        "max_epochs = 30\n",
        "results = []\n",
        "\n",
        "test_counter = 1\n",
        "H_dict = {}\n",
        "training_time_dict = {}\n",
        "model_dict = {}\n",
        "\n",
        "for params in grid:\n",
        "    print(f'Teste {test_counter}: Testando com parâmetros: {params}')\n",
        "    model = UNet(input_shape=(256, 256, 3),\n",
        "                 num_filters=(16, 32, 64, 128),\n",
        "                 kernel_size = 3,\n",
        "                 dropout_rate=0.1,\n",
        "                 val_reg=0.01)\n",
        "\n",
        "    # Compilar e treinar o modelo com os parâmetros do grid search\n",
        "    H, training_time_gpu, model = model.compile_and_train(X_train, y_train, X_val, y_val,\n",
        "                                                          max_epochs=max_epochs,\n",
        "                                                          batch_size=16,\n",
        "                                                          alpha=params['alpha'],\n",
        "                                                          beta=params['beta'],\n",
        "                                                          gamma=params['gamma'],\n",
        "                                                          penalty_weight=params['penalty_weight'])\n",
        "\n",
        "    H_dict[f'H{test_counter}'] = H\n",
        "    training_time_dict[f'training_time_gpu{test_counter}'] = training_time_gpu\n",
        "    model_dict[f'model{test_counter}'] = model\n",
        "\n",
        "    predicted_masks, inference_time_gpu = evaluate_model(model, X_val, y_val, H, training_time_gpu, max_epochs=max_epochs)\n",
        "    val_loss = H.history['val_loss'][-1]\n",
        "    val_acc = H.history['val_custom_accuracy'][-1]\n",
        "    results.append((params, val_loss, val_acc))\n",
        "    test_counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg2VYVLfyKhr",
        "outputId": "dbe958ac-9679-41d5-be4b-1511a41202e1"
      },
      "outputs": [],
      "source": [
        "# Print the results\n",
        "for result in results:\n",
        "    print(f\"Params: {result[0]}, Val Loss: {result[1]}, Val Accuracy: {result[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N5VQDIYjW1jW",
        "outputId": "7c72f0f9-b540-403b-de91-5bdb7afe7d3a"
      },
      "outputs": [],
      "source": [
        "# Convertemos os resultados em um DataFrame para facilitar a visualização\n",
        "results_df = pd.DataFrame(results, columns=['params', 'val_loss', 'val_custom_accuracy'])\n",
        "\n",
        "# Expandimos os parâmetros para colunas separadas\n",
        "params_df = pd.json_normalize(results_df['params'])\n",
        "results_df = pd.concat([params_df, results_df[['val_loss', 'val_custom_accuracy']]], axis=1)\n",
        "print(results_df)\n",
        "\n",
        "# Plotar os resultados\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Plotar perda de validação para cada parâmetro\n",
        "for i, param in enumerate(['alpha', 'beta', 'gamma', 'penalty_weight']):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    for key, grp in results_df.groupby([p for p in ['alpha', 'beta', 'gamma', 'penalty_weight'] if p != param]):\n",
        "        plt.plot(grp[param], grp['val_loss'], marker='o', label=f'Fixando {key}')\n",
        "    plt.xlabel(param)\n",
        "    plt.ylabel('Perda de Validação')\n",
        "    plt.title(f'Impacto de {param} na Perda de Validação')\n",
        "    plt.xscale('log')\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Plotar acurácia de validação para cada parâmetro\n",
        "for i, param in enumerate(['alpha', 'beta', 'gamma', 'penalty_weight']):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    for key, grp in results_df.groupby([p for p in ['alpha', 'beta', 'gamma', 'penalty_weight'] if p != param]):\n",
        "        plt.plot(grp[param], grp['val_custom_accuracy'], marker='o', label=f'Fixando {key}')\n",
        "    plt.xlabel(param)\n",
        "    plt.ylabel('Acurácia de Validação')\n",
        "    plt.title(f'Impacto de {param} na Acurácia de Validação')\n",
        "    plt.xscale('log')\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T3rSCOuj9QRv",
        "outputId": "bf8818fa-8933-4d98-df77-149db6d98737"
      },
      "outputs": [],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MFObF738bmCF",
        "outputId": "54fc11e6-6a37-446e-9937-a2f7d17c573e"
      },
      "outputs": [],
      "source": [
        "max_epochs = 30\n",
        "\n",
        "print(f'Teste sem hyperparâmetros: ')\n",
        "model = UNet(input_shape=(256, 256, 3),\n",
        "                num_filters=(16, 32, 64, 128),\n",
        "                kernel_size = 3,\n",
        "                dropout_rate=0.1,\n",
        "                val_reg=0.01)\n",
        "\n",
        "# Compilar e treinar o modelo com os parâmetros do grid search\n",
        "H_clean, training_time_gpu_clean, model_clean = model.compile_and_train(X_train, y_train, X_val, y_val,\n",
        "                                                        max_epochs=max_epochs,\n",
        "                                                        batch_size=16,\n",
        "                                                        alpha=1.0,\n",
        "                                                        beta=1.0,\n",
        "                                                        gamma=1.0,\n",
        "                                                        penalty_weight=0.001)\n",
        "\n",
        "predicted_masks, inference_time_gpu = evaluate_model(model_clean, X_val, y_val, H_clean, training_time_gpu_clean, max_epochs=max_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E33oWpUTd2Uo"
      },
      "outputs": [],
      "source": [
        "# max_epochs = 30\n",
        "\n",
        "# print(f'Teste sem hyperparâmetros: ')\n",
        "# model = UNet(input_shape=(256, 256, 3),\n",
        "#                 num_filters=(16, 32, 64, 128),\n",
        "#                 kernel_size = 3,\n",
        "#                 dropout_rate=0.1,\n",
        "#                 val_reg=0.01)\n",
        "\n",
        "# # Compilar e treinar o modelo com os parâmetros do grid search\n",
        "# H_clean, training_time_gpu_clean, model_clean = model.compile_and_train(X_train, y_train, X_val, y_val,\n",
        "#                                                         max_epochs=max_epochs,\n",
        "#                                                         batch_size=16,\n",
        "#                                                         alpha=10.0,\n",
        "#                                                         beta=1.0,\n",
        "#                                                         gamma=0.0001,\n",
        "#                                                         penalty_weight=0.001)\n",
        "\n",
        "# predicted_masks, inference_time_gpu = evaluate_model(model_clean, X_val, y_val, H_clean, training_time_gpu_clean, max_epochs=max_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_XrQw1fWtkp"
      },
      "source": [
        "## Amostragem das máscaras e IoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "50cPtSMOWIqb",
        "outputId": "a0f14c41-e79f-4866-9707-2442fa0e9280"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    # Gerar as saídas do modelo para um conjunto de entradas de teste\n",
        "    saidas_modelo = model_dict[f'model{32}'].predict(X_val)\n",
        "\n",
        "    # Iterar sobre cada saída do modelo\n",
        "    for i in range(len(X_val)):\n",
        "        # Obter a entrada correspondente e a saída real\n",
        "        img_entrada = X_val[i]\n",
        "        img_saida_real = y_val[i]\n",
        "\n",
        "        # Obter a saída gerada pelo modelo\n",
        "        img_saida_modelo = saidas_modelo[i]\n",
        "\n",
        "        # Mostrar as imagens\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(img_entrada.squeeze(), cmap='gray')\n",
        "        plt.title('Entrada')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(img_saida_real.squeeze(), cmap='gray')\n",
        "        plt.title('Saída Esperada')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(img_saida_modelo.squeeze(), cmap='gray')\n",
        "        plt.title('Saída do Modelo - GPU')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o4pT9GJiYqw"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    # Métricas do parceiro de Projeto:\n",
        "\n",
        "    # Lista para armazenar os scores de IoU\n",
        "    iou_scores = []\n",
        "    # Calcular IoUs e determinar predições corretas\n",
        "    correct_predictions = 0\n",
        "    iou_threshold = 0.5\n",
        "    for mask, result in zip(y_val, img_saida_modelo):\n",
        "        intersection = np.logical_and(mask, result)\n",
        "        union = np.logical_or(mask, result)\n",
        "        iou_score = np.sum(intersection) / np.sum(union) if np.sum(union) != 0 else 0\n",
        "        iou_scores.append(iou_score)\n",
        "        # Verificar se a predição é considerada correta (IoU >= threshold)\n",
        "        if iou_score >= iou_threshold:\n",
        "            correct_predictions += 1\n",
        "        print('IoU é: ' + str(iou_score))\n",
        "    # Calcular a média dos IoUs\n",
        "    iou_mean = np.mean(iou_scores)\n",
        "    print('Média dos IoU - GPU:', iou_mean)\n",
        "    # Calcular Coverage Ratio (CovR)\n",
        "    total_predictions = len(iou_scores)\n",
        "    covr = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    print('Coverage Ratio (CovR) - GPU:', covr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aICVl2JrxzCw"
      },
      "source": [
        "À princípio, os melhores testes encontrados com menor indício de overfitting e com as maiores acurácias foram:\n",
        "\n",
        "Teste 32: Testando com parâmetros: {'alpha': 10.0, 'beta': 1.0, 'gamma': 0.0001, 'penalty_weight': 0.001}\n",
        "\n",
        "Teste 26: Testando com parâmetros: {'alpha': 10.0, 'beta': 0.1, 'gamma': 0.0001, 'penalty_weight': 0.001}\n",
        "\n",
        "Teste 25: Testando com parâmetros: {'alpha': 10.0, 'beta': 0.1, 'gamma': 0.0001, 'penalty_weight': 0.0001}\n",
        "\n",
        "Teste 20: Testando com parâmetros: {'alpha': 1.0, 'beta': 1.0, 'gamma': 0.0001, 'penalty_weight': 0.001}\n",
        "\n",
        "Teste 17: Testando com parâmetros: {'alpha': 1.0, 'beta': 0.1, 'gamma': 0.01, 'penalty_weight': 0.0001}\n",
        "\n",
        "Teste 13: Testando com parâmetros: {'alpha': 1.0, 'beta': 0.1, 'gamma': 0.0001, 'penalty_weight': 0.0001}\n",
        "\n",
        "Teste 11: Testando com parâmetros: {'alpha': 0.1, 'beta': 1.0, 'gamma': 0.01, 'penalty_weight': 0.0001}\n",
        "\n",
        "Teste 5: Testando com parâmetros: {'alpha': 0.1, 'beta': 0.1, 'gamma': 0.01, 'penalty_weight': 0.0001}\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cQl90jW20q1D",
        "3xSP-0NwWdAq"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
