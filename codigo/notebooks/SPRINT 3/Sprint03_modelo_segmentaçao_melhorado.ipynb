{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQl90jW20q1D"
      },
      "source": [
        "# Obter dados no drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M97Z2Z1BTDYl",
        "outputId": "9c1e2878-3380-41c4-c1ae-2f7244fee68f"
      },
      "outputs": [],
      "source": [
        "!pip install pyxlsb\n",
        "!pip install -q git+https://github.com/tensorflow/examples.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehPwUJLzSsLC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from pyxlsb import open_workbook\n",
        "import random\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibpCXTgmS_Hi",
        "outputId": "fed96b58-e32f-463f-8c7e-c42a99a44640"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xSP-0NwWdAq"
      },
      "source": [
        "# Divisão de imagens e masks em amostras de treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz6Rn0h26UVx"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "masks = []\n",
        "\n",
        "for path in glob('/content/drive/Shared drives/Grupo T de Tech/Data/dataset_inteli/cropped_images/*/*'):\n",
        "  images.append(path + '/image.tif')\n",
        "  masks.append(path + '/mask.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgFIh5hrObFb"
      },
      "outputs": [],
      "source": [
        "# Função para carregar e pré-processar uma imagem e sua máscara\n",
        "def load_and_preprocess_image(image_path, mask_path, target_size):\n",
        "\n",
        "    image = load_img(image_path, target_size=target_size)\n",
        "    image = img_to_array(image) / 255.0  # Normalização entre 0 e 1\n",
        "\n",
        "    mask = load_img(mask_path, target_size=target_size, color_mode='grayscale')\n",
        "    mask = img_to_array(mask) / 255.0  # Normalização entre 0 e 1\n",
        "\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Qu_4ZJaKOTGF",
        "outputId": "6ce48d98-762e-4123-c059-9a518a9bbde4"
      },
      "outputs": [],
      "source": [
        "# Lista para armazenar imagens e máscaras pré-processadas\n",
        "images_processed = []\n",
        "masks_processed = []\n",
        "\n",
        "count = 1\n",
        "# Carregar e pré-processar todas as imagens e máscaras\n",
        "for img_path, mask_path in zip(images, masks):\n",
        "    print(count)\n",
        "    count += 1\n",
        "    img, mask = load_and_preprocess_image(img_path, mask_path, target_size=(256, 256))\n",
        "    images_processed.append(img)\n",
        "    masks_processed.append(mask)\n",
        "\n",
        "# Converter para arrays numpy\n",
        "images_processed = np.array(images_processed)\n",
        "masks_processed = np.array(masks_processed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kRApOxtFQ3IX",
        "outputId": "80f43af5-7ea3-4ecc-fe4b-f8f9c67988c4"
      },
      "outputs": [],
      "source": [
        "images_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xtU7CLmDfqL"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(images_processed, masks_processed, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dApijRqyWk_n"
      },
      "source": [
        "# Definição do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJk0H6D6T9F1"
      },
      "outputs": [],
      "source": [
        "def Unet(input_shape):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder (contraction path)\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    # Decoder (expansion path)\n",
        "    up3 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv2)\n",
        "    merge3 = layers.concatenate([conv1, up3], axis=3)\n",
        "    conv3 = layers.Conv2D(64, 3, activation='relu', padding='same')(merge3)\n",
        "    conv3 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv3)\n",
        "\n",
        "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv3)  # Saída com um canal (máscara binária)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIxkz9tyUEb8",
        "outputId": "8f96ae18-8dc3-480a-b207-49ce0b003626"
      },
      "outputs": [],
      "source": [
        "# Criar modelo U-Net\n",
        "model = Unet(input_shape=(256, 256, 3))\n",
        "#model = UNet(output_channels=1)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUl_ev0naVCN"
      },
      "outputs": [],
      "source": [
        "#forma de fazer com que o treino pare após obter uma acurácia específica\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if 'accuracy' in logs and logs.get('accuracy') is not None and logs.get('accuracy') > 0.95:\n",
        "            print(\"\\nAtingimos uma precisão maior que 95.0%, então podemos parar o treinamento!\")\n",
        "            self.model.stop_training = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZMBQc7JL15q"
      },
      "outputs": [],
      "source": [
        "# Função para calcular o Dice Coefficient\n",
        "def dice_coefficient(y_train, y_val):\n",
        "    smooth = 1e-6\n",
        "    intersection = tf.reduce_sum(y_train * y_val)\n",
        "    dice_coefficient = (2. * intersection + smooth) / (tf.reduce_sum(y_train) + tf.reduce_sum(y_val) + smooth)\n",
        "    return dice_coefficient\n",
        "\n",
        "# Função de perda de Dice\n",
        "def dice_loss(y_train, y_val):\n",
        "    return 1 - dice_coefficient(y_train, y_val)\n",
        "\n",
        "# Função para calcular a penalidade adicional\n",
        "def penalty_loss(y_train, y_val, penalty_weight):\n",
        "    # Calcular a penalidade considerando a diferença entre y_train e y_val\n",
        "    penalty = tf.reduce_sum(tf.abs(y_train - y_val))\n",
        "    # Multiplicar a penalidade pelo peso da penalidade\n",
        "    weighted_penalty = penalty_weight * penalty\n",
        "    return weighted_penalty\n",
        "\n",
        "# Função de perda combinada\n",
        "def combined_loss(y_train, y_val, penalty_weight):\n",
        "    # Perda padrão (por exemplo, perda de entropia cruzada binária)\n",
        "    standard_loss = tf.keras.losses.binary_crossentropy(y_train, y_val)\n",
        "    # Dice Loss\n",
        "    dice = dice_loss(y_train, y_val)\n",
        "    # Penalidade adicional\n",
        "    penalty = penalty_loss(y_train, y_val, penalty_weight)\n",
        "    # Perda total = perda padrão + penalidade + Dice Loss\n",
        "    total_loss = standard_loss + penalty + dice\n",
        "    return total_loss\n",
        "\n",
        "# Métrica de acurácia customizada\n",
        "def custom_accuracy(y_train, y_val):\n",
        "    # Calcular a acurácia considerando uma tolerância de 0.5 na predição\n",
        "    y_val_binary = tf.round(y_val)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_train, y_val_binary), tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "# Função para cálculo do Coverage Rate\n",
        "def coverage_rate(y_true, y_pred, threshold=0.5):\n",
        "    # Binarize the predictions\n",
        "    y_pred = tf.where(y_pred >= threshold, 1.0, 0.0)\n",
        "\n",
        "    # Calculate the coverage rate\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    total_foreground = tf.reduce_sum(y_true)\n",
        "\n",
        "    coverage = intersection / (total_foreground + tf.keras.backend.epsilon())\n",
        "\n",
        "    return coverage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K8INbM9Wpi8"
      },
      "source": [
        "# Treino do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTLb8OpzUHrj",
        "outputId": "db3cb641-7337-4e2f-daf0-37f3dbd26c8f"
      },
      "outputs": [],
      "source": [
        "callbacks = myCallback()\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam', loss=lambda y_train, y_val: combined_loss(y_train, y_val, 0.001), metrics=[custom_accuracy, coverage_rate])\n",
        "with tf.device('/gpu:0'):\n",
        "  # Treinar o modelo\n",
        "  H = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, batch_size=16, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_XrQw1fWtkp"
      },
      "source": [
        "# Avaliação do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "IpbWkEwibEV2",
        "outputId": "66ab3af7-ea86-4ccc-e5eb-1da75e53498d"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  # Avaliando o modelo\n",
        "  loss, accuracy, coverage = model.evaluate(X_val, y_val)\n",
        "  print(f'Acurácia do modelo: {accuracy}')\n",
        "\n",
        "  # Mostrando resultados\n",
        "  plt.style.use(\"ggplot\")\n",
        "  plt.figure()\n",
        "  plt.plot(H.epoch, H.history[\"loss\"], label=\"train_loss\")\n",
        "  plt.plot(H.epoch, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "  plt.plot(H.epoch, H.history[\"custom_accuracy\"], label=\"train_acc\")\n",
        "  plt.plot(H.epoch, H.history[\"val_custom_accuracy\"], label=\"val_acc\")\n",
        "  plt.title(\"Training Loss and Accuracy\")\n",
        "  plt.xlabel(\"Epoch #\")\n",
        "  plt.ylabel(\"Loss/Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCmCsYi-OtSV"
      },
      "outputs": [],
      "source": [
        "#  ZNB\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXe7NUljUoKH",
        "outputId": "ba9b7cec-f70a-40a1-9972-9698331d6288"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  # Avaliar o modelo nos dados de teste\n",
        "  results = model.evaluate(X_val, y_val)\n",
        "  print(\"Test Loss:\", results[0])\n",
        "  print(\"Test Accuracy:\", results[1])\n",
        "  print(\"Test Coverage Rate:\", results[2])\n",
        "\n",
        "  # Prever máscaras usando o modelo\n",
        "  predicted_masks = model.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "3E-mOKTEWw90",
        "outputId": "e8ca3adf-c8a3-4093-94e4-4d0ad9ec3b51"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  # Obter métricas de precisão e perda do histórico de treinamento\n",
        "  acc = H.history['custom_accuracy']\n",
        "  val_acc = H.history['val_custom_accuracy']\n",
        "  loss = H.history['loss']\n",
        "  val_loss = H.history['val_loss']\n",
        "\n",
        "  # Número de épocas\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "\n",
        "  # Plotar precisão do conjunto de treino e validação\n",
        "  plt.plot(epochs, acc, 'r', label='Precisão do Conjunto de Treino')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Precisão do Conjunto de Validação')\n",
        "  plt.title('Precisão do Conjunto de Treino e Validação')\n",
        "  plt.xlabel('Épocas')\n",
        "  plt.ylabel('Precisão')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # Plotar perda do conjunto de treino e validação\n",
        "  plt.plot(epochs, loss, 'r', label='Perda do Conjunto de Treino')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Perda do Conjunto de Validação')\n",
        "  plt.title('Perda do Conjunto de Treino e Validação')\n",
        "  plt.xlabel('Épocas')\n",
        "  plt.ylabel('Perda')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "50cPtSMOWIqb",
        "outputId": "f84c0c3c-0513-4c85-f02a-0eef20d6c51c"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    # Gerar as saídas do modelo para um conjunto de entradas de teste\n",
        "    saidas_modelo = model.predict(X_val)\n",
        "    imgs_preditas = []\n",
        "\n",
        "    # Iterar sobre cada saída do modelo\n",
        "    for i in range(len(X_val)):\n",
        "        # Obter a entrada correspondente e a saída real\n",
        "        img_entrada = X_val[i]\n",
        "        img_saida_real = y_val[i]\n",
        "\n",
        "        # Obter a saída gerada pelo modelo\n",
        "        img_saida_modelo = saidas_modelo[i]\n",
        "\n",
        "        #Aplicar limiarização apenas durante a inferência\n",
        "        img_saida_modelo_limiarizada = np.where(img_saida_modelo < 0.5, 0, 1)\n",
        "\n",
        "        imgs_preditas.append(img_saida_modelo_limiarizada)\n",
        "\n",
        "        # Mostrar as imagens\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(img_entrada)\n",
        "        plt.title('Entrada')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(img_saida_real, cmap='gray')\n",
        "        plt.title('Saída Esperada')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(img_saida_modelo_limiarizada, cmap='gray')\n",
        "        plt.title('Saída do Modelo (Limiarizada)')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxRzXSJGfymO"
      },
      "outputs": [],
      "source": [
        "iou_scores = []\n",
        "with tf.device('/gpu:0'):\n",
        "    for mask, result in zip(y_val, imgs_preditas):\n",
        "        intersection = np.logical_and(mask, result)\n",
        "        union = np.logical_or(mask, result)\n",
        "\n",
        "        iou_score = np.sum(intersection) / np.sum(union)\n",
        "        iou_scores.append(iou_score)\n",
        "\n",
        "iou_mean = np.mean(iou_scores)\n",
        "print('Média dos IoU:', iou_mean)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
