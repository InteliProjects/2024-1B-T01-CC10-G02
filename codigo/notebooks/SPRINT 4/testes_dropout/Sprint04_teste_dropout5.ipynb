{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQl90jW20q1D"
      },
      "source": [
        "# Obter dados no drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehPwUJLzSsLC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras import layers, models, Input, regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibpCXTgmS_Hi",
        "outputId": "f9e6d9db-80ff-43a4-d271-c619400fe6a5"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xSP-0NwWdAq"
      },
      "source": [
        "# Carregamento de dados - Sem Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz6Rn0h26UVx"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "masks = []\n",
        "\n",
        "for path in glob('/content/drive/Shared drives/Grupo T de Tech/Data/dataset_inteli/cropped_images/*/*'):\n",
        "  images.append(path + '/image.tif')\n",
        "  masks.append(path + '/mask.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgFIh5hrObFb"
      },
      "outputs": [],
      "source": [
        "# Função para carregar e pré-processar uma imagem e sua máscara\n",
        "def load_and_preprocess_image(image_path, mask_path, target_size):\n",
        "\n",
        "    image = load_img(image_path, target_size=target_size)\n",
        "    image = img_to_array(image) / 255.0  # Normalização entre 0 e 1\n",
        "\n",
        "    mask = load_img(mask_path, target_size=target_size, color_mode='grayscale')\n",
        "    mask = img_to_array(mask) / 255.0  # Normalização entre 0 e 1\n",
        "\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qu_4ZJaKOTGF"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    # Lista para armazenar imagens e máscaras pré-processadas\n",
        "    images_processed = []\n",
        "    masks_processed = []\n",
        "\n",
        "    # Carregar e pré-processar todas as imagens e máscaras\n",
        "    for img_path, mask_path in zip(images, masks):\n",
        "        img, mask = load_and_preprocess_image(img_path, mask_path, target_size=(256, 256))\n",
        "        images_processed.append(img)\n",
        "        masks_processed.append(mask)\n",
        "\n",
        "    # Converter para arrays numpy\n",
        "    images_processed = np.array(images_processed)\n",
        "    masks_processed = np.array(masks_processed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1XJsmj6Iu1W"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(images_processed, masks_processed, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dApijRqyWk_n"
      },
      "source": [
        "# Definição de parâmetros e modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZMBQc7JL15q"
      },
      "outputs": [],
      "source": [
        "# Função para calcular a sigmoide e converter para 0 ou 1 o output\n",
        "class ThresholdLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        return tf.where(inputs < 0.5, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCfQonXrv6nG"
      },
      "outputs": [],
      "source": [
        "# Métrica de acurácia customizada\n",
        "def custom_accuracy(y_train, y_val):\n",
        "    # Calcular a acurácia considerando uma tolerância de 0.5 na predição\n",
        "    y_val_binary = tf.round(y_val)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_train, y_val_binary), tf.float32))\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJk0H6D6T9F1"
      },
      "outputs": [],
      "source": [
        "def Unet(input_shape, dropout_rate):\n",
        "    num_filters=(16, 32, 64, 128)\n",
        "    kernel_size=3\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder (contraction path)\n",
        "    conv1 = layers.Conv2D(num_filters[0], kernel_size, activation='relu', padding='same')(inputs)\n",
        "    conv1 = layers.Conv2D(num_filters[0], kernel_size, activation='relu', padding='same')(conv1)\n",
        "    drop1 = layers.Dropout(dropout_rate)(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(drop1)\n",
        "\n",
        "    conv2 = layers.Conv2D(num_filters[1], kernel_size, activation='relu', padding='same')(pool1)\n",
        "    conv2 = layers.Conv2D(num_filters[1], kernel_size, activation='relu', padding='same')(conv2)\n",
        "    drop2 = layers.Dropout(dropout_rate)(conv2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(drop2)\n",
        "\n",
        "    conv3 = layers.Conv2D(num_filters[2], kernel_size, activation='relu', padding='same')(pool2)\n",
        "    conv3 = layers.Conv2D(num_filters[2], kernel_size, activation='relu', padding='same')(conv3)\n",
        "    drop3 = layers.Dropout(dropout_rate)(conv3)\n",
        "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(drop3)\n",
        "\n",
        "    # Bottleneck\n",
        "    conv4 = layers.Conv2D(num_filters[3], kernel_size, activation='relu', padding='same')(pool3)\n",
        "    conv4 = layers.Conv2D(num_filters[3], kernel_size, activation='relu', padding='same')(conv4)\n",
        "    drop4 = layers.Dropout(dropout_rate)(conv4)\n",
        "\n",
        "    # Decoder (expansion path)\n",
        "    up5 = layers.Conv2DTranspose(num_filters[2], (2, 2), strides=(2, 2), padding='same')(drop4)\n",
        "    merge5 = layers.concatenate([conv3, up5], axis=3)\n",
        "    conv5 = layers.Conv2D(num_filters[2], kernel_size, activation='relu', padding='same')(merge5)\n",
        "    conv5 = layers.Conv2D(num_filters[2], kernel_size, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    up6 = layers.Conv2DTranspose(num_filters[1], (2, 2), strides=(2, 2), padding='same')(conv5)\n",
        "    merge6 = layers.concatenate([conv2, up6], axis=3)\n",
        "    conv6 = layers.Conv2D(num_filters[1], kernel_size, activation='relu', padding='same')(merge6)\n",
        "    conv6 = layers.Conv2D(num_filters[1], kernel_size, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = layers.Conv2DTranspose(num_filters[0], (2, 2), strides=(2, 2), padding='same')(conv6)\n",
        "    merge7 = layers.concatenate([conv1, up7], axis=3)\n",
        "    conv7 = layers.Conv2D(num_filters[0], kernel_size, activation='relu', padding='same')(merge7)\n",
        "    conv7 = layers.Conv2D(num_filters[0], kernel_size, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv7)  # Saída com um canal (máscara binária)\n",
        "\n",
        "    threshold_output = ThresholdLayer()(outputs)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWjsU-GQPCIZ"
      },
      "source": [
        "# Testes com dropout de 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unbZArxBcdqZ"
      },
      "outputs": [],
      "source": [
        "def train_model():\n",
        "  model = Unet(input_shape=(256, 256, 3), dropout_rate=0.5)\n",
        "\n",
        "  with tf.device('/gpu:0'):\n",
        "      # Compilar o modelo\n",
        "      model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=[custom_accuracy])\n",
        "\n",
        "      # Calcula o tempo de treino\n",
        "      start_time = time.time()\n",
        "\n",
        "      # Definições\n",
        "      max_epochs = 50\n",
        "      batch_size = 16\n",
        "\n",
        "      # Treinar o modelo\n",
        "      H = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=max_epochs, batch_size=batch_size)\n",
        "\n",
        "      # Obter métricas de precisão e perda do histórico de treinamento\n",
        "      acc = H.history['custom_accuracy']\n",
        "      val_acc = H.history['val_custom_accuracy']\n",
        "      loss = H.history['loss']\n",
        "      val_loss = H.history['val_loss']\n",
        "\n",
        "      # Número de épocas\n",
        "      epochs = range(1, len(acc) + 1)\n",
        "\n",
        "      # Calcula o tempo de inferência\n",
        "      start_time = time.time()\n",
        "\n",
        "      # Prever máscaras usando o modelo\n",
        "      predicted_masks = model.predict(X_val)\n",
        "\n",
        "      # Lista para armazenar os scores de IoU\n",
        "      iou_scores = []\n",
        "      correct_predictions = 0\n",
        "      iou_threshold = 0.5\n",
        "      for mask, result in zip(y_val, predicted_masks):\n",
        "          intersection = np.logical_and(mask, result)\n",
        "          union = np.logical_or(mask, result)\n",
        "          iou_score = np.sum(intersection) / np.sum(union) if np.sum(union) != 0 else 0\n",
        "          iou_scores.append(iou_score)\n",
        "          if iou_score >= iou_threshold:\n",
        "              correct_predictions += 1\n",
        "\n",
        "      iou_mean = np.mean(iou_scores)\n",
        "\n",
        "      total_predictions = len(iou_scores)\n",
        "      covr = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "  return [model,iou_mean,covr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77dhv950PH-F",
        "outputId": "9c122449-0da8-446b-fcda-7e45cbeb1bc3"
      },
      "outputs": [],
      "source": [
        "# Criar modelo U-Net\n",
        "total_loss = 0\n",
        "total_acc = 0\n",
        "total_iou = 0\n",
        "total_covr = 0\n",
        "for i in range(4):\n",
        "\n",
        "  print(\"Início do teste \", i+1)\n",
        "  model, iou_mean, covr = train_model()\n",
        "\n",
        "  results = model.evaluate(X_val, y_val)\n",
        "\n",
        "  total_loss += results[0]\n",
        "  total_acc += results[1]\n",
        "  total_iou += iou_mean\n",
        "  total_covr += covr\n",
        "\n",
        "print(\"A perda média foi de \", (total_loss/4))\n",
        "print(\"A acurácia média foi de \", (total_acc/4))\n",
        "print(\"O iou médio foi de \", (total_iou/4))\n",
        "print(\"O covr médio foi de \", (total_covr/4))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
