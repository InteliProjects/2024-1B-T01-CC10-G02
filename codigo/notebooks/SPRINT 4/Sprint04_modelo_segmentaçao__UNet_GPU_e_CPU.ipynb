{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQl90jW20q1D"
      },
      "source": [
        "# Obter dados no drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehPwUJLzSsLC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras import layers, models, Input, regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibpCXTgmS_Hi",
        "outputId": "c0609b7d-0f53-4f77-ae7e-2d8674869bc7"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xSP-0NwWdAq"
      },
      "source": [
        "# Carregamento de dados - Sem Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz6Rn0h26UVx"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "masks = []\n",
        "\n",
        "for path in glob('/content/drive/Shared drives/Grupo T de Tech/Data/dataset_inteli/cropped_images/*/*'):\n",
        "  images.append(path + '/image.tif')\n",
        "  masks.append(path + '/mask.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgFIh5hrObFb"
      },
      "outputs": [],
      "source": [
        "# Função para carregar e pré-processar uma imagem e sua máscara\n",
        "def load_and_preprocess_image(image_path, mask_path, target_size):\n",
        "\n",
        "    image = load_img(image_path, target_size=target_size)\n",
        "    image = img_to_array(image) / 255.0  # Normalização entre 0 e 1\n",
        "\n",
        "    mask = load_img(mask_path, target_size=target_size, color_mode='grayscale')\n",
        "    mask = img_to_array(mask) / 255.0  # Normalização entre 0 e 1\n",
        "\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qu_4ZJaKOTGF"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    # Lista para armazenar imagens e máscaras pré-processadas\n",
        "    images_processed = []\n",
        "    masks_processed = []\n",
        "\n",
        "    # Carregar e pré-processar todas as imagens e máscaras\n",
        "    for img_path, mask_path in zip(images, masks):\n",
        "        img, mask = load_and_preprocess_image(img_path, mask_path, target_size=(256, 256))\n",
        "        images_processed.append(img)\n",
        "        masks_processed.append(mask)\n",
        "\n",
        "    # Converter para arrays numpy\n",
        "    images_processed = np.array(images_processed)\n",
        "    masks_processed = np.array(masks_processed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kRApOxtFQ3IX",
        "outputId": "a4c8374a-d9c6-47e2-f456-5cb582c97dcf"
      },
      "outputs": [],
      "source": [
        "images_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1XJsmj6Iu1W"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(images_processed, masks_processed, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dApijRqyWk_n"
      },
      "source": [
        "# Definição de parâmetros e modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JcNGKwDxX94"
      },
      "outputs": [],
      "source": [
        "class CyclicLR(Callback):\n",
        "    def __init__(self, base_lr=1e-4, max_lr=1e-3, step_size=2000., mode='triangular'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.iterations = 0\n",
        "        self.history = {}\n",
        "\n",
        "    def clr(self):\n",
        "        cycle = np.floor(1 + self.iterations / (2 * self.step_size))\n",
        "        x = np.abs(self.iterations / self.step_size - 2 * cycle + 1)\n",
        "        lr = self.base_lr + (self.max_lr - self.base_lr) * max(0, (1 - x))\n",
        "        if self.mode == 'triangular2':\n",
        "            lr = lr / float(2 ** (cycle - 1))\n",
        "        elif self.mode == 'exp_range':\n",
        "            lr = lr * (0.999 ** self.iterations)\n",
        "        return lr\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        logs = logs or {}\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        self.iterations += 1\n",
        "        lr = self.clr()\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
        "        self.history.setdefault('lr', []).append(lr)\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "# Função de callbacks\n",
        "def get_callbacks():\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "    clr = CyclicLR(base_lr=1e-4, max_lr=1e-3, step_size=2000., mode='triangular2')\n",
        "    return [early_stopping, reduce_lr, clr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZMBQc7JL15q"
      },
      "outputs": [],
      "source": [
        "# Função para calcular a sigmoide e converter para 0 ou 1 o output\n",
        "class ThresholdLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        return tf.where(inputs < 0.5, 0.0, 1.0)\n",
        "\n",
        "# Função para calcular o Dice Coefficient\n",
        "def dice_coefficient(y_train, y_val):\n",
        "    smooth = 1e-6\n",
        "    intersection = tf.reduce_sum(y_train * y_val)\n",
        "    dice_coefficient = (2. * intersection + smooth) / (tf.reduce_sum(y_train) + tf.reduce_sum(y_val) + smooth)\n",
        "    return dice_coefficient\n",
        "\n",
        "# Função de perda de Dice\n",
        "def dice_loss(y_train, y_val):\n",
        "    return 1 - dice_coefficient(y_train, y_val)\n",
        "\n",
        "# Função para calcular a penalidade adicional\n",
        "def penalty_loss(y_train, y_val, penalty_weight):\n",
        "    # Calcular a penalidade considerando a diferença entre y_train e y_val\n",
        "    penalty = tf.reduce_sum(tf.abs(y_train - y_val))\n",
        "    # Multiplicar a penalidade pelo peso da penalidade\n",
        "    weighted_penalty = penalty_weight * penalty\n",
        "    return weighted_penalty\n",
        "\n",
        "# Função de perda combinada\n",
        "def combined_loss(y_train, y_val, penalty_weight):\n",
        "    # Perda padrão (por exemplo, perda de entropia cruzada binária)\n",
        "    standard_loss = tf.keras.losses.binary_crossentropy(y_train, y_val)\n",
        "    # Dice Loss\n",
        "    dice = dice_loss(y_train, y_val)\n",
        "    # Penalidade adicional\n",
        "    penalty = penalty_loss(y_train, y_val, penalty_weight)\n",
        "    # Perda total = perda padrão + penalidade + Dice Loss\n",
        "    total_loss = standard_loss + penalty + dice\n",
        "    return total_loss\n",
        "\n",
        "# Métrica de acurácia customizada\n",
        "def custom_accuracy(y_train, y_val):\n",
        "    # Calcular a acurácia considerando uma tolerância de 0.5 na predição\n",
        "    y_val_binary = tf.round(y_val)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_train, y_val_binary), tf.float32))\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJk0H6D6T9F1"
      },
      "outputs": [],
      "source": [
        "class UNet:\n",
        "    def __init__(self, input_shape, num_filters, kernel_size, dropout_rate, val_reg):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.val_reg = val_reg\n",
        "\n",
        "    def build_model(self):\n",
        "        inputs = tf.keras.Input(shape=self.input_shape)\n",
        "        reg = regularizers.L2(self.val_reg)\n",
        "\n",
        "        # Encoder (contraction path)\n",
        "        conv1 = layers.Conv2D(self.num_filters[0], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(inputs)\n",
        "        conv1 = layers.Conv2D(self.num_filters[0], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(conv1)\n",
        "        drop1 = layers.Dropout(self.dropout_rate)(conv1)\n",
        "        pool1 = layers.MaxPooling2D(pool_size=(2, 2))(drop1)\n",
        "\n",
        "        conv2 = layers.Conv2D(self.num_filters[1], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(pool1)\n",
        "        conv2 = layers.Conv2D(self.num_filters[1], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(conv2)\n",
        "        drop2 = layers.Dropout(self.dropout_rate)(conv2)\n",
        "        pool2 = layers.MaxPooling2D(pool_size=(2, 2))(drop2)\n",
        "\n",
        "        conv3 = layers.Conv2D(self.num_filters[2], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(pool2)\n",
        "        conv3 = layers.Conv2D(self.num_filters[2], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(conv3)\n",
        "        drop3 = layers.Dropout(self.dropout_rate)(conv3)\n",
        "        pool3 = layers.MaxPooling2D(pool_size=(2, 2))(drop3)\n",
        "\n",
        "        # Bottleneck\n",
        "        conv4 = layers.Conv2D(self.num_filters[3], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(pool3)\n",
        "        conv4 = layers.Conv2D(self.num_filters[3], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(conv4)\n",
        "        drop4 = layers.Dropout(self.dropout_rate)(conv4)\n",
        "\n",
        "        # Decoder (expansion path)\n",
        "        up5 = layers.Conv2DTranspose(self.num_filters[2], (2, 2), strides=(2, 2), padding='same')(drop4)\n",
        "        merge5 = layers.concatenate([conv3, up5], axis=3)\n",
        "        conv5 = layers.Conv2D(self.num_filters[2], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(merge5)\n",
        "        conv5 = layers.Conv2D(self.num_filters[2], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(conv5)\n",
        "\n",
        "        up6 = layers.Conv2DTranspose(self.num_filters[1], (2, 2), strides=(2, 2), padding='same')(conv5)\n",
        "        merge6 = layers.concatenate([conv2, up6], axis=3)\n",
        "        conv6 = layers.Conv2D(self.num_filters[1], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(merge6)\n",
        "        conv6 = layers.Conv2D(self.num_filters[1], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(conv6)\n",
        "\n",
        "        up7 = layers.Conv2DTranspose(self.num_filters[0], (2, 2), strides=(2, 2), padding='same')(conv6)\n",
        "        merge7 = layers.concatenate([conv1, up7], axis=3)\n",
        "        conv7 = layers.Conv2D(self.num_filters[0], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(merge7)\n",
        "        conv7 = layers.Conv2D(self.num_filters[0], self.kernel_size, activation='relu', padding='same', kernel_regularizer=reg)(conv7)\n",
        "\n",
        "        outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv7)  # Saída com um canal (máscara binária)\n",
        "\n",
        "        threshold_output = ThresholdLayer()(outputs)\n",
        "\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "    def compile_and_train(self, X_train, y_train, X_val, y_val, max_epochs, batch_size):\n",
        "        model = self.build_model()\n",
        "        callbacks = get_callbacks()\n",
        "\n",
        "        # Compilar o modelo\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                      loss=lambda y_train, y_val: combined_loss(y_train, y_val, 0.001),\n",
        "                      metrics=[custom_accuracy])\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Treinar o modelo\n",
        "        H = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                      epochs=max_epochs, batch_size=batch_size, callbacks=callbacks)\n",
        "\n",
        "        # Salvar o tempo de treino\n",
        "        training_time_gpu = time.time() - start_time\n",
        "\n",
        "        return H, training_time_gpu, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0HalkJyHT-9"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_val, y_val, H, training_time_gpu, max_epochs):\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Plotar perda e acurácia de treinamento e validação\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(H.epoch, H.history[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(H.epoch, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(H.epoch, H.history[\"custom_accuracy\"], label=\"train_acc\")\n",
        "    plt.plot(H.epoch, H.history[\"val_custom_accuracy\"], label=\"val_acc\")\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    results = model.evaluate(X_val, y_val)\n",
        "\n",
        "    inference_time_gpu = time.time() - start_time\n",
        "\n",
        "    print(\"Test Loss:\", results[0])\n",
        "    print(\"Test Accuracy:\", results[1])\n",
        "\n",
        "    # Prever máscaras usando o modelo\n",
        "    predicted_masks = model.predict(X_val)\n",
        "\n",
        "    # Obter métricas de precisão e perda do treinamento\n",
        "    acc = H.history['custom_accuracy']\n",
        "    val_acc = H.history['val_custom_accuracy']\n",
        "    loss = H.history['loss']\n",
        "    val_loss = H.history['val_loss']\n",
        "\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    # Plotar precisão do conjunto\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, acc, 'r', label='Precisão do Conjunto de Treino')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Precisão do Conjunto de Validação')\n",
        "    plt.title('Precisão do Conjunto de Treino e Validação')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Precisão')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotar perda do conjunto\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, loss, 'r', label='Perda do Conjunto de Treino')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Perda do Conjunto de Validação')\n",
        "    plt.title('Perda do Conjunto de Treino e Validação')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Perda')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print('Tempo de treino GPU (segundos): ', training_time_gpu)\n",
        "    print('Tempo de treino GPU por época (segundos): ', training_time_gpu / max_epochs)\n",
        "    print('Tempo de inferência GPU (segundos): ', inference_time_gpu)\n",
        "\n",
        "    # Plotando o tempo de treinamento e inferência\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(['Training'], [training_time_gpu], color='orange', label='Training Time')\n",
        "    plt.bar(['Inference'], [inference_time_gpu], color='blue', label='Inference Time')\n",
        "    plt.ylabel('Time (seconds)')\n",
        "    plt.title('Training and Inference Time')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return predicted_masks, inference_time_gpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIxkz9tyUEb8"
      },
      "outputs": [],
      "source": [
        "max_epochs = 200\n",
        "\n",
        "unet = UNet(input_shape = (256, 256, 3),\n",
        "            num_filters=(16, 32, 64, 128),\n",
        "            kernel_size = 3,\n",
        "            dropout_rate = 0.3,\n",
        "            val_reg = 0.001)\n",
        "\n",
        "# Treinando o modelo\n",
        "H, training_time_gpu, model = unet.compile_and_train(X_train, y_train, X_val, y_val,\n",
        "                                                     max_epochs = max_epochs,\n",
        "                                                     batch_size = 16)\n",
        "\n",
        "# Avaliando o modelo\n",
        "predicted_masks, inference_time_gpu = evaluate_model(model, X_val, y_val, H, training_time_gpu, max_epochs=max_epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_XrQw1fWtkp"
      },
      "source": [
        "## Amostragem das máscaras e IoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50cPtSMOWIqb"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    # Gerar as saídas do modelo para um conjunto de entradas de teste\n",
        "    saidas_modelo = model.predict(X_val)\n",
        "\n",
        "    # Iterar sobre cada saída do modelo\n",
        "    for i in range(len(X_val)):\n",
        "        # Obter a entrada correspondente e a saída real\n",
        "        img_entrada = X_val[i]\n",
        "        img_saida_real = y_val[i]\n",
        "\n",
        "        # Obter a saída gerada pelo modelo\n",
        "        img_saida_modelo = saidas_modelo[i]\n",
        "\n",
        "        # Mostrar as imagens\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(img_entrada.squeeze(), cmap='gray')\n",
        "        plt.title('Entrada')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(img_saida_real.squeeze(), cmap='gray')\n",
        "        plt.title('Saída Esperada')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(img_saida_modelo.squeeze(), cmap='gray')\n",
        "        plt.title('Saída do Modelo - GPU')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o4pT9GJiYqw",
        "outputId": "f88287a4-c162-48bb-aaed-2913c963d928"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    # Métricas do parceiro de Projeto:\n",
        "\n",
        "    # Lista para armazenar os scores de IoU\n",
        "    iou_scores = []\n",
        "    # Calcular IoUs e determinar predições corretas\n",
        "    correct_predictions = 0\n",
        "    iou_threshold = 0.5\n",
        "    for mask, result in zip(y_val, img_saida_modelo):\n",
        "        intersection = np.logical_and(mask, result)\n",
        "        union = np.logical_or(mask, result)\n",
        "        iou_score = np.sum(intersection) / np.sum(union) if np.sum(union) != 0 else 0\n",
        "        iou_scores.append(iou_score)\n",
        "        # Verificar se a predição é considerada correta (IoU >= threshold)\n",
        "        if iou_score >= iou_threshold:\n",
        "            correct_predictions += 1\n",
        "        print('IoU é: ' + str(iou_score))\n",
        "    # Calcular a média dos IoUs\n",
        "    iou_mean = np.mean(iou_scores)\n",
        "    print('Média dos IoU - GPU:', iou_mean)\n",
        "    # Calcular Coverage Ratio (CovR)\n",
        "    total_predictions = len(iou_scores)\n",
        "    covr = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    print('Coverage Ratio (CovR) - GPU:', covr)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
